{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from config import model_name\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from os import path\n",
    "import sys\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import importlib\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# model_name: str = 'NRMS'\n",
    "model_name: str = 'NAML'\n",
    "# model_name: str = 'TANR'\n",
    "# model_name: str = 'LSTUR'\n",
    "# model_name: str = 'DKN'\n",
    "# model_name: str = 'HiFiArk'\n",
    "# model_name: str = 'Exp1'\n",
    "\n",
    "try:\n",
    "    Model = getattr(importlib.import_module(f\"model.{model_name}\"), model_name)\n",
    "    config = getattr(importlib.import_module('config'), f\"{model_name}Config\")\n",
    "except AttributeError:\n",
    "    print(f\"{model_name} not included!\")\n",
    "    exit()\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "RESULT_CSV = 'results.csv'\n",
    "norm = lambda x: (x-np.min(x)) / (np.max(x)-np.min(x))\n",
    "standardization= lambda x: (x-np.mean(x)) / (np.std(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_score(y_true, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    gains = 2**y_true - 1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10):\n",
    "    best = dcg_score(y_true, y_true, k)\n",
    "    actual = dcg_score(y_true, y_score, k)\n",
    "    return actual / best\n",
    "\n",
    "\n",
    "def mrr_score(y_true, y_score):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order)\n",
    "    rr_score = y_true / (np.arange(len(y_true)) + 1)\n",
    "    return np.sum(rr_score) / np.sum(y_true)\n",
    "\n",
    "\n",
    "def value2rank(d):\n",
    "    values = list(d.values())\n",
    "    ranks = [sorted(values, reverse=True).index(x) for x in values]\n",
    "    return {k: ranks[i] + 1 for i, k in enumerate(d.keys())}\n",
    "\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Load news for evaluation.\n",
    "    \"\"\"\n",
    "    def __init__(self, news_path):\n",
    "        super(NewsDataset, self).__init__()\n",
    "        self.news_parsed = pd.read_table(\n",
    "            news_path,\n",
    "            usecols=['id'] + config.dataset_attributes['news'],\n",
    "            converters={\n",
    "                attribute: literal_eval\n",
    "                for attribute in set(config.dataset_attributes['news']) & set([\n",
    "                    'title', 'abstract', 'title_entities', 'abstract_entities'\n",
    "                ])\n",
    "            })\n",
    "        self.news2dict = self.news_parsed.to_dict('index')\n",
    "        for key1 in self.news2dict.keys():\n",
    "            for key2 in self.news2dict[key1].keys():\n",
    "                if type(self.news2dict[key1][key2]) != str:\n",
    "                    self.news2dict[key1][key2] = torch.tensor(\n",
    "                        self.news2dict[key1][key2])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.news_parsed)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.news2dict[idx]\n",
    "        return item\n",
    "\n",
    "\n",
    "class UserDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Load users for evaluation, duplicated rows will be dropped\n",
    "    \"\"\"\n",
    "    def __init__(self, behaviors_path, user2int_path):\n",
    "        super(UserDataset, self).__init__()\n",
    "        self.behaviors = pd.read_table(behaviors_path,\n",
    "                                       header=None,\n",
    "                                       usecols=[1, 3],\n",
    "                                       names=['user', 'clicked_news'])\n",
    "        self.behaviors.clicked_news.fillna(' ', inplace=True)\n",
    "        self.behaviors.drop_duplicates(inplace=True)\n",
    "        user2int = dict(pd.read_table(user2int_path).values.tolist())\n",
    "        user_total = 0\n",
    "        user_missed = 0\n",
    "        for row in self.behaviors.itertuples():\n",
    "            user_total += 1\n",
    "            if row.user in user2int:\n",
    "                self.behaviors.at[row.Index, 'user'] = user2int[row.user]\n",
    "            else:\n",
    "                user_missed += 1\n",
    "                self.behaviors.at[row.Index, 'user'] = 0\n",
    "        if model_name == 'LSTUR':\n",
    "            print(f'User miss rate: {user_missed/user_total:.4f}')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.behaviors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.behaviors.iloc[idx]\n",
    "        item = {\n",
    "            \"user\":\n",
    "            row.user,\n",
    "            \"clicked_news_string\":\n",
    "            row.clicked_news,\n",
    "            \"clicked_news\":\n",
    "            row.clicked_news.split()[:config.num_clicked_news_a_user]\n",
    "        }\n",
    "        item['clicked_news_length'] = len(item[\"clicked_news\"])\n",
    "        repeated_times = config.num_clicked_news_a_user - len(\n",
    "            item[\"clicked_news\"])\n",
    "        assert repeated_times >= 0\n",
    "        item[\"clicked_news\"] = ['PADDED_NEWS'\n",
    "                                ] * repeated_times + item[\"clicked_news\"]\n",
    "\n",
    "        return item\n",
    "\n",
    "\n",
    "class BehaviorsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Load behaviors for evaluation, (user, time) pair as session\n",
    "    \"\"\"\n",
    "    def __init__(self, behaviors_path):\n",
    "        super(BehaviorsDataset, self).__init__()\n",
    "        self.behaviors = pd.read_table(behaviors_path,\n",
    "                                       header=None,\n",
    "                                       usecols=range(5),\n",
    "                                       names=[\n",
    "                                           'impression_id', 'user', 'time',\n",
    "                                           'clicked_news', 'impressions'\n",
    "                                       ])\n",
    "        self.behaviors.clicked_news.fillna(' ', inplace=True)\n",
    "        self.behaviors.impressions = self.behaviors.impressions.str.split()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.behaviors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.behaviors.iloc[idx]\n",
    "        item = {\n",
    "            \"impression_id\": row.impression_id,\n",
    "            \"user\": row.user,\n",
    "            \"time\": row.time,\n",
    "            \"clicked_news_string\": row.clicked_news,\n",
    "            \"impressions\": row.impressions\n",
    "        }\n",
    "        return item\n",
    "\n",
    "\n",
    "def calculate_single_user_metric(pair):\n",
    "    try:\n",
    "        auc = roc_auc_score(*pair)\n",
    "        mrr = mrr_score(*pair)\n",
    "        ndcg5 = ndcg_score(*pair, 5)\n",
    "        ndcg10 = ndcg_score(*pair, 10)\n",
    "        return [auc, mrr, ndcg5, ndcg10]\n",
    "    except ValueError:\n",
    "        return [np.nan] * 4\n",
    "\n",
    "def sigmoid(x):\n",
    "    sig = 1 / (1 + np.exp(-x))\n",
    "    return sig\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, directory, num_workers, max_count=sys.maxsize, mode='test'):\n",
    "    \"\"\"\n",
    "    Evaluate model on target directory.\n",
    "    Args:\n",
    "        model: model to be evaluated\n",
    "        directory: the directory that contains two files (behaviors.tsv, news_parsed.tsv)\n",
    "        num_workers: processes number for calculating metrics\n",
    "    Returns:\n",
    "        AUC\n",
    "        MRR\n",
    "        nDCG@5\n",
    "        nDCG@10\n",
    "    \"\"\"\n",
    "    news_dataset = NewsDataset(path.join(directory, 'news_parsed.tsv'))\n",
    "    news_dataloader = DataLoader(news_dataset,\n",
    "                                 batch_size=config.batch_size * 16,\n",
    "                                 shuffle=False,\n",
    "                                 num_workers=config.num_workers,\n",
    "                                 drop_last=False,\n",
    "                                 pin_memory=True)\n",
    "\n",
    "    news2vector = {}\n",
    "    for minibatch in tqdm(news_dataloader,\n",
    "                          desc=\"Calculating vectors for news\"):\n",
    "        news_ids = minibatch[\"id\"]\n",
    "        if any(id not in news2vector for id in news_ids):\n",
    "            news_vector = model.get_news_vector(minibatch)\n",
    "            for id, vector in zip(news_ids, news_vector):\n",
    "                if id not in news2vector:\n",
    "                    news2vector[id] = vector\n",
    "\n",
    "    news2vector['PADDED_NEWS'] = torch.zeros(\n",
    "        list(news2vector.values())[0].size())\n",
    "\n",
    "    user_dataset = UserDataset(path.join(directory, 'behaviors.tsv'),\n",
    "                               'data/train/user2int.tsv')\n",
    "    user_dataloader = DataLoader(user_dataset,\n",
    "                                 batch_size=config.batch_size * 16,\n",
    "                                 shuffle=False,\n",
    "                                 num_workers=config.num_workers,\n",
    "                                 drop_last=False,\n",
    "                                 pin_memory=True)\n",
    "\n",
    "    user2vector = {}\n",
    "    for minibatch in tqdm(user_dataloader,\n",
    "                          desc=\"Calculating vectors for users\"):\n",
    "        user_strings = minibatch[\"clicked_news_string\"]\n",
    "        if any(user_string not in user2vector for user_string in user_strings):\n",
    "            clicked_news_vector = torch.stack([\n",
    "                torch.stack([news2vector[x].to(device) for x in news_list],\n",
    "                            dim=0) for news_list in minibatch[\"clicked_news\"]\n",
    "            ],\n",
    "                                              dim=0).transpose(0, 1)\n",
    "            if model_name == 'LSTUR':\n",
    "                user_vector = model.get_user_vector(\n",
    "                    minibatch['user'], minibatch['clicked_news_length'],\n",
    "                    clicked_news_vector)\n",
    "            else:\n",
    "                user_vector = model.get_user_vector(clicked_news_vector)\n",
    "            for user, vector in zip(user_strings, user_vector):\n",
    "                if user not in user2vector:\n",
    "                    user2vector[user] = vector\n",
    "\n",
    "    behaviors_dataset = BehaviorsDataset(path.join(directory, 'behaviors.tsv'))\n",
    "    behaviors_dataloader = DataLoader(behaviors_dataset,\n",
    "                                      batch_size=1,\n",
    "                                      shuffle=False,\n",
    "                                      num_workers=config.num_workers)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    tasks = []\n",
    "    result_dict = {}\n",
    "\n",
    "    for minibatch in tqdm(behaviors_dataloader,\n",
    "                          desc=\"Calculating probabilities\"):\n",
    "        count += 1\n",
    "        if count == max_count:\n",
    "            break\n",
    "\n",
    "        candidate_news_vector = torch.stack([\n",
    "            news2vector[news[0].split('-')[0]]\n",
    "            for news in minibatch['impressions']\n",
    "        ],\n",
    "                                            dim=0)\n",
    "        user_vector = user2vector[minibatch['clicked_news_string'][0]]\n",
    "        click_probability = model.get_prediction(candidate_news_vector,\n",
    "                                                 user_vector)\n",
    "\n",
    "        y_pred = click_probability.tolist()\n",
    "        if mode == 'train':\n",
    "            y_true = [\n",
    "                int(news[0].split('-')[1]) for news in minibatch['impressions']\n",
    "            ]\n",
    "            tasks.append((y_true, y_pred))\n",
    "        elif mode == 'test':\n",
    "            # result_dict[f'{count-1}'] = norm(y_pred)\n",
    "            result_dict[f'{count-1}'] = sigmoid(standardization(y_pred)*2)\n",
    "\n",
    "\n",
    "    if mode == 'train':\n",
    "        with Pool(processes=num_workers) as pool:\n",
    "            results = pool.map(calculate_single_user_metric, tasks)\n",
    "\n",
    "        aucs, mrrs, ndcg5s, ndcg10s = np.array(results).T\n",
    "        return np.nanmean(aucs), np.nanmean(mrrs), np.nanmean(ndcg5s), np.nanmean(\n",
    "            ndcg10s)\n",
    "    elif mode == 'test':\n",
    "        return result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:3\n",
      "Evaluating model NAML\n",
      "Load saved parameters in ./checkpoint/NAML/ckpt-18000.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671963b6a43147498787f0908194d217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating vectors for news:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c926e7c8244e3e94c19338bfaba907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating vectors for users:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e69313d32ac48aa89b01f96da61c1cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating probabilities:   0%|          | 0/28531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7495\n",
      "MRR: 0.4211\n",
      "nDCG@5: 0.5060\n",
      "nDCG@10: 0.5951\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71bb83d36e1d4196bdbc0642c1dbcc5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating vectors for news:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9fa0072accc4022ac31d8abd04dbf09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating vectors for users:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c215602bb69a447189823b64eb524666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating probabilities:   0%|          | 0/46332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Using device:', device)\n",
    "print(f'Evaluating model {model_name}')\n",
    "# Don't need to load pretrained word/entity/context embedding\n",
    "# since it will be loaded from checkpoint later\n",
    "model = Model(config).to(device)\n",
    "from train import latest_checkpoint  # Avoid circular imports\n",
    "checkpoint_path = latest_checkpoint(path.join('./checkpoint', model_name))\n",
    "if checkpoint_path is None:\n",
    "    print('No checkpoint file found!')\n",
    "    exit()\n",
    "print(f\"Load saved parameters in {checkpoint_path}\")\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "auc, mrr, ndcg5, ndcg10 = evaluate(model, './data/val',\n",
    "                                   config.num_workers, mode='train')\n",
    "print(\n",
    "    f'AUC: {auc:.4f}\\nMRR: {mrr:.4f}\\nnDCG@5: {ndcg5:.4f}\\nnDCG@10: {ndcg10:.4f}'\n",
    ")\n",
    "\n",
    "y_preds = evaluate(model, './data/test', config.num_workers, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>p5</th>\n",
       "      <th>p6</th>\n",
       "      <th>p7</th>\n",
       "      <th>p8</th>\n",
       "      <th>p9</th>\n",
       "      <th>p10</th>\n",
       "      <th>p11</th>\n",
       "      <th>p12</th>\n",
       "      <th>p13</th>\n",
       "      <th>p14</th>\n",
       "      <th>p15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.986740</td>\n",
       "      <td>0.246305</td>\n",
       "      <td>0.154075</td>\n",
       "      <td>0.924738</td>\n",
       "      <td>0.191706</td>\n",
       "      <td>0.330019</td>\n",
       "      <td>0.423013</td>\n",
       "      <td>0.127046</td>\n",
       "      <td>0.546368</td>\n",
       "      <td>0.137391</td>\n",
       "      <td>0.884011</td>\n",
       "      <td>0.469321</td>\n",
       "      <td>0.081894</td>\n",
       "      <td>0.327081</td>\n",
       "      <td>0.963361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.655668</td>\n",
       "      <td>0.690927</td>\n",
       "      <td>0.029469</td>\n",
       "      <td>0.024686</td>\n",
       "      <td>0.817341</td>\n",
       "      <td>0.569102</td>\n",
       "      <td>0.330013</td>\n",
       "      <td>0.971381</td>\n",
       "      <td>0.885688</td>\n",
       "      <td>0.846261</td>\n",
       "      <td>0.388972</td>\n",
       "      <td>0.073607</td>\n",
       "      <td>0.854497</td>\n",
       "      <td>0.386028</td>\n",
       "      <td>0.279750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.513867</td>\n",
       "      <td>0.964572</td>\n",
       "      <td>0.668355</td>\n",
       "      <td>0.020455</td>\n",
       "      <td>0.946087</td>\n",
       "      <td>0.238195</td>\n",
       "      <td>0.666692</td>\n",
       "      <td>0.083654</td>\n",
       "      <td>0.877423</td>\n",
       "      <td>0.862080</td>\n",
       "      <td>0.357508</td>\n",
       "      <td>0.755734</td>\n",
       "      <td>0.390520</td>\n",
       "      <td>0.170110</td>\n",
       "      <td>0.075322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.878759</td>\n",
       "      <td>0.089822</td>\n",
       "      <td>0.564866</td>\n",
       "      <td>0.475850</td>\n",
       "      <td>0.920324</td>\n",
       "      <td>0.926570</td>\n",
       "      <td>0.367967</td>\n",
       "      <td>0.579773</td>\n",
       "      <td>0.006860</td>\n",
       "      <td>0.818645</td>\n",
       "      <td>0.062779</td>\n",
       "      <td>0.454696</td>\n",
       "      <td>0.497693</td>\n",
       "      <td>0.477989</td>\n",
       "      <td>0.865106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.116177</td>\n",
       "      <td>0.536326</td>\n",
       "      <td>0.303768</td>\n",
       "      <td>0.834703</td>\n",
       "      <td>0.070747</td>\n",
       "      <td>0.281333</td>\n",
       "      <td>0.966931</td>\n",
       "      <td>0.826439</td>\n",
       "      <td>0.536488</td>\n",
       "      <td>0.076838</td>\n",
       "      <td>0.048714</td>\n",
       "      <td>0.978014</td>\n",
       "      <td>0.729237</td>\n",
       "      <td>0.672510</td>\n",
       "      <td>0.372146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46327</th>\n",
       "      <td>46327</td>\n",
       "      <td>0.063778</td>\n",
       "      <td>0.181381</td>\n",
       "      <td>0.963281</td>\n",
       "      <td>0.807094</td>\n",
       "      <td>0.204593</td>\n",
       "      <td>0.701495</td>\n",
       "      <td>0.076892</td>\n",
       "      <td>0.652247</td>\n",
       "      <td>0.413439</td>\n",
       "      <td>0.707882</td>\n",
       "      <td>0.210484</td>\n",
       "      <td>0.966948</td>\n",
       "      <td>0.943172</td>\n",
       "      <td>0.096983</td>\n",
       "      <td>0.212084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46328</th>\n",
       "      <td>46328</td>\n",
       "      <td>0.020985</td>\n",
       "      <td>0.935554</td>\n",
       "      <td>0.637090</td>\n",
       "      <td>0.121343</td>\n",
       "      <td>0.968655</td>\n",
       "      <td>0.376734</td>\n",
       "      <td>0.657071</td>\n",
       "      <td>0.867992</td>\n",
       "      <td>0.909637</td>\n",
       "      <td>0.255468</td>\n",
       "      <td>0.265553</td>\n",
       "      <td>0.820601</td>\n",
       "      <td>0.253967</td>\n",
       "      <td>0.089985</td>\n",
       "      <td>0.226558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46329</th>\n",
       "      <td>46329</td>\n",
       "      <td>0.485513</td>\n",
       "      <td>0.087318</td>\n",
       "      <td>0.919288</td>\n",
       "      <td>0.766375</td>\n",
       "      <td>0.129334</td>\n",
       "      <td>0.526088</td>\n",
       "      <td>0.114406</td>\n",
       "      <td>0.180489</td>\n",
       "      <td>0.992796</td>\n",
       "      <td>0.075720</td>\n",
       "      <td>0.526058</td>\n",
       "      <td>0.503796</td>\n",
       "      <td>0.814883</td>\n",
       "      <td>0.196966</td>\n",
       "      <td>0.821418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46330</th>\n",
       "      <td>46330</td>\n",
       "      <td>0.919966</td>\n",
       "      <td>0.044079</td>\n",
       "      <td>0.475451</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.560428</td>\n",
       "      <td>0.316280</td>\n",
       "      <td>0.199902</td>\n",
       "      <td>0.829922</td>\n",
       "      <td>0.048178</td>\n",
       "      <td>0.229163</td>\n",
       "      <td>0.995133</td>\n",
       "      <td>0.679140</td>\n",
       "      <td>0.453993</td>\n",
       "      <td>0.423944</td>\n",
       "      <td>0.500328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46331</th>\n",
       "      <td>46331</td>\n",
       "      <td>0.657948</td>\n",
       "      <td>0.454178</td>\n",
       "      <td>0.437178</td>\n",
       "      <td>0.080195</td>\n",
       "      <td>0.914809</td>\n",
       "      <td>0.994768</td>\n",
       "      <td>0.175168</td>\n",
       "      <td>0.456131</td>\n",
       "      <td>0.482214</td>\n",
       "      <td>0.054617</td>\n",
       "      <td>0.091162</td>\n",
       "      <td>0.631029</td>\n",
       "      <td>0.779839</td>\n",
       "      <td>0.732650</td>\n",
       "      <td>0.220689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46332 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index        p1        p2        p3        p4        p5        p6  \\\n",
       "0          0  0.986740  0.246305  0.154075  0.924738  0.191706  0.330019   \n",
       "1          1  0.655668  0.690927  0.029469  0.024686  0.817341  0.569102   \n",
       "2          2  0.513867  0.964572  0.668355  0.020455  0.946087  0.238195   \n",
       "3          3  0.878759  0.089822  0.564866  0.475850  0.920324  0.926570   \n",
       "4          4  0.116177  0.536326  0.303768  0.834703  0.070747  0.281333   \n",
       "...      ...       ...       ...       ...       ...       ...       ...   \n",
       "46327  46327  0.063778  0.181381  0.963281  0.807094  0.204593  0.701495   \n",
       "46328  46328  0.020985  0.935554  0.637090  0.121343  0.968655  0.376734   \n",
       "46329  46329  0.485513  0.087318  0.919288  0.766375  0.129334  0.526088   \n",
       "46330  46330  0.919966  0.044079  0.475451  0.420455  0.560428  0.316280   \n",
       "46331  46331  0.657948  0.454178  0.437178  0.080195  0.914809  0.994768   \n",
       "\n",
       "             p7        p8        p9       p10       p11       p12       p13  \\\n",
       "0      0.423013  0.127046  0.546368  0.137391  0.884011  0.469321  0.081894   \n",
       "1      0.330013  0.971381  0.885688  0.846261  0.388972  0.073607  0.854497   \n",
       "2      0.666692  0.083654  0.877423  0.862080  0.357508  0.755734  0.390520   \n",
       "3      0.367967  0.579773  0.006860  0.818645  0.062779  0.454696  0.497693   \n",
       "4      0.966931  0.826439  0.536488  0.076838  0.048714  0.978014  0.729237   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "46327  0.076892  0.652247  0.413439  0.707882  0.210484  0.966948  0.943172   \n",
       "46328  0.657071  0.867992  0.909637  0.255468  0.265553  0.820601  0.253967   \n",
       "46329  0.114406  0.180489  0.992796  0.075720  0.526058  0.503796  0.814883   \n",
       "46330  0.199902  0.829922  0.048178  0.229163  0.995133  0.679140  0.453993   \n",
       "46331  0.175168  0.456131  0.482214  0.054617  0.091162  0.631029  0.779839   \n",
       "\n",
       "            p14       p15  \n",
       "0      0.327081  0.963361  \n",
       "1      0.386028  0.279750  \n",
       "2      0.170110  0.075322  \n",
       "3      0.477989  0.865106  \n",
       "4      0.672510  0.372146  \n",
       "...         ...       ...  \n",
       "46327  0.096983  0.212084  \n",
       "46328  0.089985  0.226558  \n",
       "46329  0.196966  0.821418  \n",
       "46330  0.423944  0.500328  \n",
       "46331  0.732650  0.220689  \n",
       "\n",
       "[46332 rows x 16 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_to_submit = pd.DataFrame(y_preds).T\n",
    "# results_to_submit.columns = [\"p1\", \"p2\", \"p3\", \"p4\", \"p5\", \"p6\", \"p7\", \"p8\", \"p9\", \"p10\", \"p11\", \"p12\", \"p13\", \"p14\", \"p15\"]\n",
    "# results_to_submit\n",
    "\n",
    "results_to_submit.to_csv(\n",
    "  'results.csv',\n",
    "  header=[\"p1\", \"p2\", \"p3\", \"p4\", \"p5\", \"p6\", \"p7\", \"p8\", \"p9\", \"p10\", \"p11\", \"p12\", \"p13\", \"p14\", \"p15\"],\n",
    "  index_label='index'\n",
    "  )\n",
    "pd.read_csv(RESULT_CSV, )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定義需要的log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import BaseConfig\n",
    "\n",
    "num_epochs = BaseConfig.num_epochs\n",
    "# Number of batchs to show loss\n",
    "num_batches_show_loss = BaseConfig.num_batches_show_loss\n",
    "# Number of batchs to check metrics on validation dataset\n",
    "num_batches_validate = BaseConfig.num_batches_validate\n",
    "batch_size = BaseConfig.batch_size\n",
    "learning_rate = BaseConfig.learning_rate\n",
    "# Number of workers for data loading\n",
    "num_workers = BaseConfig.num_workers\n",
    "# Number of sampled click history for each user\n",
    "num_clicked_news_a_user = BaseConfig.num_clicked_news_a_user\n",
    "num_words_title = BaseConfig.num_words_title\n",
    "num_words_abstract = BaseConfig.num_words_abstract\n",
    "word_freq_threshold = BaseConfig.word_freq_threshold\n",
    "entity_freq_threshold = BaseConfig.entity_freq_threshold\n",
    "entity_confidence_threshold = BaseConfig.entity_confidence_threshold\n",
    "# K\n",
    "negative_sampling_ratio = BaseConfig.negative_sampling_ratio\n",
    "dropout_probability = BaseConfig.dropout_probability\n",
    "# Modify the following by the output of `src/dataprocess.py`\n",
    "num_words = BaseConfig.num_words\n",
    "num_categories = BaseConfig.num_categories\n",
    "num_entities = BaseConfig.num_entities\n",
    "num_users = BaseConfig.num_users\n",
    "word_embedding_dim = BaseConfig.word_embedding_dim\n",
    "category_embedding_dim = BaseConfig.category_embedding_dim\n",
    "# Modify the following only if you use another dataset\n",
    "entity_embedding_dim = BaseConfig.entity_embedding_dim\n",
    "# For additive attention\n",
    "query_vector_dim = BaseConfig.query_vector_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle competitions submit -c 2023-datamining-final-project -f results.csv -m \"[NAML] AUC: 0.7495, MRR: 0.4211, nDCG@5: 0.5060, nDCG@10: 0.5951  *EXTRA: [num_epochs=80, batch_size=768, learning_rate=0.0001, num_clicked_news_a_user=50, num_words_title=20, num_words_abstract=50, word_freq_threshold=1, entity_freq_threshold=2, entity_confidence_threshold=0.5, negative_sampling_ratio=2, dropout_probability=0.2, ]iter=18000, stadandardization+sigmoid.\"\n"
     ]
    }
   ],
   "source": [
    "EXTRA_MSG: str = ('' + \\\n",
    "  # f'SMOTE+RANDOM stacking ' + \\\n",
    "  f'{num_epochs=}, '\n",
    "  f'{batch_size=}, '\n",
    "  f'{learning_rate=}, '\n",
    "  f'{num_clicked_news_a_user=}, '\n",
    "  f'{num_words_title=}, '\n",
    "  f'{num_words_abstract=}, '\n",
    "  f'{word_freq_threshold=}, '\n",
    "  f'{entity_freq_threshold=}, '\n",
    "  f'{entity_confidence_threshold=}, '\n",
    "  f'{negative_sampling_ratio=}, '\n",
    "  f'{dropout_probability=}, '\n",
    "  # f'take away age>=90 from training data ' + \\\n",
    "  # f'ratio=(8, 2) ' + \\\n",
    "  # f'with normalization ({norm_mode=}) ' + \\\n",
    "  # f'Logistic Regression!' + \\\n",
    "  '')\n",
    "\n",
    "# if REMOVE_MISMATCH:\n",
    "#   EXTRA_MSG += f' | {REMOVE_MISMATCH=}, '\n",
    "# if REFINE_CAPITAL_DIFF:\n",
    "#   EXTRA_MSG += f' | {REFINE_CAPITAL_DIFF=}, '\n",
    "# if REFINE_AGE:\n",
    "#   EXTRA_MSG += f' | {REFINE_AGE=}, '\n",
    "# if REFINE_HPWEEK:\n",
    "#   EXTRA_MSG += f' | {REFINE_HPWEEK=}, '\n",
    "# if REFINE_RACE:\n",
    "#   EXTRA_MSG += f' | {REFINE_RACE=}, '\n",
    "\n",
    "log = (\n",
    "  f\"kaggle competitions submit -c 2023-datamining-final-project -f {RESULT_CSV} -m \"\n",
    "  # f'''\"Features: {best_config['feature']}. INFO: '''\n",
    "  f'''\"[{model_name}] AUC: {auc:.4f}, MRR: {mrr:.4f}, nDCG@5: {ndcg5:.4f}, nDCG@10: {ndcg10:.4f}''' \n",
    "  # [Acc={acc:.4f}, iteration={best_config['iteration']}, lr={best_config['lr']:.6f}, {l2_lambda=:.3f}] \n",
    "  f'''  *EXTRA: [{EXTRA_MSG}]iter=18000, stadandardization+sigmoid.\"'''\n",
    ")\n",
    "print(log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submmit to the Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.13 / client 1.5.12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13.1M/13.1M [00:04<00:00, 3.01MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted to 2023 Data Mining Final Project"
     ]
    }
   ],
   "source": [
    "# For safty.\n",
    "import os\n",
    "# raise KeyError('Are you sure you want to submit the result?')\n",
    "_ = os.system(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
